# -*- coding: utf-8 -*-
"""activations_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wj3TG3NIOaaVNjnGcSSIcoEU5te_7f5_
"""

# === 4. WRAPPER CLASS AND MODEL INSTRUMENTATION ===
print("\n--- Preparing model for activation extraction... ---")

class BlockOutputWrapper(torch.nn.Module):
    """A wrapper class to intercept and save the hidden states of a model layer."""
    def __init__(self, block):
        super().__init__()
        self.block = block
        self.last_hidden_state = None

    def forward(self, *args, **kwargs):
        output = self.block(*args, **kwargs)
        self.last_hidden_state = output[0]
        return output

    def reset(self):
        self.last_hidden_state = None

# Wrap each layer of the model with our custom wrapper.
try:
    layers_path = model.model.layers
    for i, layer in enumerate(layers_path):
        layers_path[i] = BlockOutputWrapper(layer)
    print(f"Wrapped {len(layers_path)} layers successfully.")
except AttributeError:
    print("Could not find 'model.model.layers'. Please adjust the path for your model's architecture.")
    # Exit or raise error if layers can't be found.
    # For this script, we'll assume the path is correct and continue.


# === 5. DATA LOADING ===
def load_prompts_from_file(filename, field_name):
    """Loads a list of prompts from a specified field in a JSONL file."""
    prompts_list = []
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                if field_name in data:
                    prompts_list.append(data[field_name])
            except json.JSONDecodeError:
                print(f"Warning: Skipping malformed line in {filename}")
    return prompts_list

print(f"\n--- Loading prompts from {jsonl_filename}... ---")
prompts_to_process = load_prompts_from_file(jsonl_filename, prompt_field)
print(f"Loaded {len(prompts_to_process)} prompts to process.")


# --- Main Extraction Loop ---
for i, prompt in enumerate(tqdm(prompts_to_process, desc="Extracting Activations")):

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        model(**inputs)

    prompt_activations = {}
    for layer_idx in layers_to_extract:
        wrapped_layer = model.model.layers[layer_idx]

        # --- THIS IS THE CORRECTED LINE ---
        # We now use 2 indices for a 2D tensor to get the last token's activations.
        last_token_activations = wrapped_layer.last_hidden_state[-1, :].cpu()

        prompt_activations[layer_idx] = last_token_activations

        wrapped_layer.reset()

    save_path = os.path.join(output_dir, f"prompt_{i}_activations.pt")
    torch.save(prompt_activations, save_path)

    gc.collect()
    torch.cuda.empty_cache()

print("\n--- Activation extraction complete! ---")

!zip -r /content/activations.zip /content/extracted_activations_test_2025-08-11

!pip install transformers torch bitsandbytes accelerate

import torch
import json
import os
from tqdm import tqdm
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM

# === 2. CONFIGURATION ===
# --- Model and File Settings ---
# Set the ID of the model you want to load from Hugging Face.
model_id = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
# The name of your JSONL file containing the prompts.
jsonl_filename = '/content/mmlu_psychology_forward_pass_prompts.jsonl'
# The name of the field inside your JSONL that contains the prompt text.
prompt_field = "forward_pass_prompt"
# The directory where the output activation files will be saved.
output_dir = "extracted_activations_test_2025-08-11"

# --- Extraction Settings ---
# Define which layers to extract activations from.
# Use 'list(range(32))' for all 32 layers of the DeepSeek-8B model.
# Or use 'list(range(10, 32))' for layers 10 through 31.
layers_to_extract = [15]


# === 3. LOAD MODEL AND TOKENIZER ===
print(f"--- Loading model: {model_id} ---")
# Set up the device to use the GPU if available.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the tokenizer.
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Load the model with 4-bit quantization to save memory.
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"  # Automatically uses the GPU.
)
model.eval() # Set the model to evaluation mode.
print("--- Model and tokenizer loaded successfully. ---")